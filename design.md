# log 设计

第一个需要设计的就是每一条日志实际所存储的形式，因为 `serde` 只是一个 序列化和反序列化库，并不能够从 `log` 中区分出每一条日志，所以需要设计 `log` 的格式来区分，这里参考 `bitcask` 的设计，将每条 `log` 设计成下面的形式：

```
| kv_size(4B) | serialized kv |
```

当 `version` 为 0 的时候代表被删除，正常的 `version` 都是大于 0 的正整数，读取每一条记录的时候，先读取 8B 的内容，判断要读取多少内容到 buffer 中来，然后再反序列化

现在要关注的就是是否把前面的 `meta` 信息也放到 `struct` 中去，如果放到其中去，那么就不能够正常的区分不同的 `log`，也就是先有鸡还是先有蛋的问题，所以只能每次都序列化的外面，
但是 `version` 是可以放到 序列化的内容中去的，这是因为放到这个里面的话，也和序列化本身无关了

# get set rm passed 1.0

1.0 跑通测试，非常拙的一个版本实现，我的想法是先跑起来，然后再进行优化，其实也主要是对 Rust 也不太熟悉，所以先跑通测试，然后再优化，现在的实现是这样的：

1. 日志存储格式为上述的 log 格式
2. set：
    每次写入的时候，直接 `append` 到文件的最后，先写入序列化之后的长度 4B，然后写入序列化之后的 kv, 每次都只是写入新的，version 为 1，大于 0 的一个整数
3. rm：
    和 set 一样，只不过将 `version` 设置为 0
4. get：
    每次都将所有的 kv 读入到内存的 map 中，如果遇到 `version` 为 0 的，那么就认为是删除的，将这个 kv 从 map 中删除掉

目前的实现非常拙，来进行第四步的优化，内存 map 中只保存所有的 key 以及 文件中的 `offset`，每次更新的时候，先写入到 log 中，成功之后再更新内存中的 map 数据结构

注意当前只有一个 `file handler` 来控制所有的读写操作，现在其实可以将读写区分开，写的永远只写到文件的末尾，读的可以随时更改 `offset`

手动记录每个 kv 的 offset？在第一次读入的时候就记住？
如果是按需读入，也可以进行记录的，这样看来也是可行的，只记录下自己需要读取的进入到内存中，这样看来好像消耗也不是很大，如果有个 key 不在当前的里面，也只能顺序来读，但是每次都必须读到文件的最后，因为可能后面还有新的

## 一种思路 
那么另一个优化思路就是类似 bitcask 来做，将索引信息也持久化保存起来

初始化的时候读取索引信息进入到内存中来，之后就只用更新所需要的 key，但是写索引信息和写wal哪个应该在前面呢？应该先写 wal，再写索引，这样的话，如果 索引没写进去，也认为是没有写入成功

那么 set 的过程变成了：
1. 写 kv 到文件中
2. 写 index 到索引文件中
3. 更新内存中的索引

只有这三步完全成功，才算是写入成功，但是如果保证原子性呢？如果在写 kv 成功了，但是索引没写成功，也认为是失败？
那如果 kv 写成功了，那么这些 key 就算是丢失了，永远不会被 gc 掉了，将会永远保存，
这样有什么影响吗？
1. 对于之后的 get，没有影响，认为不存在，因为索引文件中并没有
2. 对于之后的 set，没有影响，这个时候会记录新的值
3. 对于之后的 rm，没有影响，这个和 get 是一样的
4. 对于 compaction，有影响，因为会重新建立索引文件，这个时候会认为这个 key 存在了，所以不能这么做，不能这么做的原因就是不能保存写 kv 和 写 index 的原子性

看来是不行的，得重新思考，先不做这种优化，先不持久化索引文件，

## 另一种思路
初始化的时候就读取所有的文件到索引中来，然后构建出内存索引，之后更新的时候先写盘，然后更新内存索引

# compaction

## 朴素思路1
上头了，一个简单朴素的思路如下：
每个 log 文件大小最大大小为 1MB，如果写入超过 1MB，就新开一个文件来写，每当一个新的文件写完的时候，进行一次 compaction, 需要和之前所有的 log 文件来进行 compaction

每次都写入文件到 1MB，触发 compact，进行一次 compaction

compaction 的过程描述：

1. 内存中保存一个索引结构，`<key, <file_idx, offset>>` 已经保存了所有的 key 已经 对应文件的 log pointer，所以只需要将索引中存在的 kv 刷一遍到盘上即可

2. 根据索引 找到 来重新写入到临时文件中去，每个临时文件也只写 1MB，超过 1MB 就新开一个文件写，直到全部写完，注意这个时候可以顺便直接新的索引直接记录到内存中

3. 删除之前的 sst，并且 rename 刚才临时文件成最终使用的 log

4. 重新设定好 writer， write_index 等信息

存在的问题：

1. 如何判断需要 compaction 的时机？
    如果实现成 leveldb 的 LSM tree 的形式，当某一层的文件数量超过多少的时候才进行 compaction 的话，目前看来有点复杂

2. 如何进行 compaction，是否一次读取所有的文件，是否会出现把内存打满的情况？
    如果按照每个 key 平均 20bytes 来算，那么一个 kv 就是 20 + 8 + 4 = 32 bytes，
    8G 内存一共可以保存 268435456 个 key 在内存中，看起来这个并不是瓶颈

3. 问题是，如果正在 compaction 的时候，是否允许读写？
    1. 第一版实现可以直接 block 住，阻塞所有的读写进程，同步进行
    2. 之后可以考虑使得 compaction 和前台行为分离开

具体实现：

1. 文件名命名改为 `sst_{number}`，代表当前写了几个 sst 了，每次打开一个目录的时候，先遍历当前目录下所有的 `sst` file，建立起索引，并且记录下当前已经写到了第几个 `sst` 了

2. 每次 set rm 都只写到最新的 sst，并且每次写完检查一下当前文件的大小是不是超过 1MB 了，如果超过，就触发 `compaction`

## 朴素思路1的改进


# 并发

##  Creating a shared KvsEngine

感觉 Rust 的安全就在于此，如果实现了 Clone 之后，拿到 immutatable 的对象，就不用担心多线程的的非同步读写操作了，然后使用 `Arc<Mutex<T>>` 来保存需要修改的接口，每次在使用之前一定要先拿到锁才行，其实可以用读写锁更好


## what if when panic?

当某个线程 panic 的时候，直接再复制一个当前这个线程出来，这个时候就需要把原来接受任务的 receiver 传递到线程中去，要保存这个，只要根据这个就能够拿到新的任务了！

# Lock-free

如果想要实现 lock free, 可以考虑把 kvstore 改写一下, 目前根据 lock free 本身的 project 说明，可以来进行优化





压缩策略：
* 使用 uncompacted 参数, 用来触发 compact, 代表，每当写入 100 条重复的 key 时候来进行 compact

lock free:
思路：
使用 rwlock 保护 index，get 和 set 都拿读锁

* get 的时候，先拿到 读锁，然后知道要读取的文件，使用 map 保存需要读取的文件，当返回的时候把这个文件从 map 中删除掉，map 为

set 的时候是 append only 的，直到真的


